<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>  Robostats Wiki | Intro to Reinforcement Learning</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/robostats-wiki/assets/img/favicon.ico">
<link rel="stylesheet" href="/robostats-wiki/assets/css/main.css">

<link rel="canonical" href="/robostats-wiki/blog/2020/intro_to_RL/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/robostats-wiki/assets/js/distillpub/template.v2.js"></script>
    <script src="/robostats-wiki/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Intro to Reinforcement Learning",
      "description": "Introduction to markov decision process, policies, and reinforcement learning.",
      "published": "2020-12-20 00:00:00 -0500",
      "authors": [
        
        {
          "author": "Daqian Cheng",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Robotics Institute, Carnegie Mellon University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/robostats-wiki/">
       <span class="font-weight-bold"></span>   Robostats Wiki
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/robostats-wiki/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/robostats-wiki/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Intro to Reinforcement Learning</h1>
        <p>Introduction to markov decision process, policies, and reinforcement learning.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h1 id="markov-decision-process-mdp-and-reinforcement-learning-rl">Markov Decision Process (MDP) and Reinforcement Learning (RL)</h1>

<p><strong>RL vs Bandits</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Similarities</th>
      <th>Differences</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Possibly stochastic environment</td>
      <td>Bandits: <strong>single-step</strong> decision making</td>
    </tr>
    <tr>
      <td>Evaluative feedback</td>
      <td>RL: <strong>sequential</strong> decision making</td>
    </tr>
  </tbody>
</table>

<h2 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h2>

<h3 id="mdp-concepts-and-definitions">MDP Concepts and Definitions</h3>

<p><strong>Markov</strong>: given the present state, the future and the past are independent.<br />
For MDP, action outcomes only depend on the current state: \(P(S_{t+1} = s'| S_t = s_t, A_t = a_t, S_{t-1} = s_{t-1}, A_{t-1} = a_{t-1}, \dots) = P(S_{t+1} = s'| S_t = s_t, A_t = a_t)\)</p>

<p>An <strong>MDP</strong> is defined by:</p>
<ul>
  <li>A set of states \(s \in S\)</li>
  <li>A set of actions \(a\in A\)</li>
  <li>A transition function \(T(s, a, s')\) of transition probabilities</li>
  <li>A reward function \(r(s, a, s')\)</li>
  <li>A start state \(s_0\), or a distribution \(\mu_0(s)\)</li>
</ul>

<p>A <strong>Trajectory</strong>, a.k.a. an episode, is a sequence of states and actions.</p>

<h3 id="reward-and-goal">Reward and Goal</h3>

<p><strong>Return</strong> is the sum of rewards: \(R=\sum_{i=0}^\infty r_i\)</p>

<p><strong>Goal</strong> is to find the actions that maximize the expected return: \(\max_{a_0,\dots,a_t} E[R]\)<br />
This goal does not capture:</p>
<ul>
  <li>Constraints</li>
  <li>Maximize reward for <strong>any</strong> point along a trajectory</li>
  <li>Maximize reward at <strong>some timestep t</strong> along a trajectory</li>
</ul>

<h2 id="policies">Policies</h2>

<p>For any MDP, a <strong>policy</strong> maps a state to an action or a distribution over actions: \(\pi: S\rightarrow A\).<br />
<strong>Optimal policy \(\pi^*\)</strong>: maximizes expected return. Goal of RL is to find the optimal policy.</p>

<h3 id="alternative-return">Alternative Return</h3>

<p>To avoid infinite return with infinite time steps, we can use</p>
<ul>
  <li>Discounted reward: \(R(\pi) = \lim _{T\rightarrow \infty} E[\sum_i^T \gamma^i r_i]\), where \(\gamma\in(0, 1)\) imposes a sense of time horizon.</li>
  <li>Average reward: \(\bar{r}(\pi) = \lim _{T\rightarrow \infty} \frac{1}{T} E[\sum_i^T r_i]\). This reward is better for cases of function approximation.</li>
</ul>

<h3 id="ranking-policies">Ranking Policies</h3>

<p>Define <strong>value function</strong>:</p>

\[V_\pi(s) = \lim _{T\rightarrow \infty} E[\sum_i^T \gamma^i r_i|s_0=s, a_{0:t-1}\sim \pi]\]

<p>To be better at a given state \(s\): \(V_\pi(s) \geq V_{\pi'}(s)\)<br />
To be universally better: \(V_\pi(s) \geq V_{\pi'}(s)\ \forall s\)<br />
Policies have a <strong>partial ordering</strong>: it is possible to have two policies that \(V_\pi(s_1) \geq V_{\pi'}(s_1)\) and \(V_\pi(s_2) \leq V_{\pi'}(s_2)\), therefore neither is better than the other.</p>

<h3 id="optimal-policy">Optimal Policy</h3>

<p>A single best policy \(\pi^*\) exists when there is no function approximation;<br />
A single best policy \(\pi^*\) <strong>may not</strong> exist when there is function approximation;<br />
Why? Because two states with different optimal actions may be approximated with the same feature value, and in such cases there does not exist a single policy that satisfies both states. <br />
What can we do instead?</p>
<ul>
  <li>Find optimal policy for a particular start-state \(s_0\).</li>
  <li>Find optimal policy for a particular start-state distribution \(\mu_0(s)\).</li>
  <li>Find optimal policy for the stationary distribution, which exists and does not depend on the start-state when the MDP is ergodic. This option does not reset the robot to a start-state, and is related to the average reward formulation.</li>
</ul>

\[d_\pi(s)=\lim_{T\rightarrow \infty}P(s_T=s|a_{0:t-1}\sim \pi)\]

<p>For this course, it is assumed that the robot is reset to some initial state distribution \(\mu_0(s)\), and we optimize for the expected discounted sum of rewards.</p>

<h2 id="reinforcement-learning-methods">Reinforcement Learning Methods</h2>

<ul>
  <li><strong>Model-based</strong>
    <ul>
      <li>Sample efficient, best for learning in real-world</li>
      <li>Sometimes give worse performance</li>
      <li>Usually more interpretable</li>
      <li>Easier to debug</li>
      <li>Easier to encode prior information</li>
      <li>Most generalizable to new environment</li>
    </ul>
  </li>
  <li><strong>Q-function based</strong>
    <ul>
      <li>In the middle of model-based and gradient-based methods.</li>
    </ul>
  </li>
  <li><strong>Gradient-based</strong>
    <ul>
      <li>Least efficient</li>
      <li>Gives best performing policies</li>
      <li>Fewest hyper-parameters to tune</li>
      <li>Best for sim2real transfer</li>
    </ul>
  </li>
</ul>

<p><strong>Combinations</strong> of the above methods are also possible, which often give the best performance with the fewest samples but require more tuning of hyper-parameters.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2020   Robostats Wiki.
    
    
    Last updated: December 22, 2020.
    
  </div>
</footer>



  </body>

  <d-bibliography src="/robostats-wiki/assets/bibliography/">
  </d-bibliography>

</html>
