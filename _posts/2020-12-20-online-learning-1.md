---
layout: distill
title: Online Learning
description: Greedy, Halving, Randomized Greedy
date: 2020-12-20

authors:
  - name: Shengcao Cao
    affiliations:
      name: Robotics Institute, Carnegie Mellon University

bibliography: 2018-12-20-online-learning-1.bib

---

## Introduction

Let's first consider an example to get some intuition about **online learning**. Imagine you are new to a city, and you want to explore the local restaurants. Every Saturday, you can select a restaurant based on your experience and the menus from restaurants, and try the food there. You may feel satisfactory when the food suits your taste, or sometimes suffer from the horrible food.

Online learning may show you a great strategy to find the best restaurants. We use these terms in online learning:

- In this example, you are acting as the *online learner*.
- At each timestep $t$ (Saturday), the online learner can obtain an *observation* $x^{(t)}$ (menus) from the *environment*.
- After that, the online learner makes a *prediction* $\hat y^{(t)}$ (restaurant).
- Then the environment will return the online learner a *loss* $l^{(t)}$ (how bad the food is) based on its prediction.
- Sometimes, a *true answer* $y^{(t)}$ is returned in the end of this timestep as well, which in this case should be the best restaurant in hindsight. The loss can be defined as a function of the true answer and the prediction $$l^{(t)}=l(\hat y^{(t)},y^{(t)})$$, e.g. zero-one loss.
- The goal of the online learner is to *minimize the total loss* $$\sum_tl^{(t)}$$ (have tried the best food) of the entire process. Please note that in the context of online learning, we often talk about loss, while in reinforcement learning we use the term of reward. Minimizing the loss is equivalent to maximizing the reward.

Another thing to notice is that everything is changing in the environment. Your taste, the menus, and the quality of food from restaurants may not always stay the same. An expert foodie's choices should be dynamic. In robot learning we often face the **explorationâ€“exploitation** dilemma. In our example, exploration means trying a new restaurant which you have never been to, and exploitation means going back to a restaurant where you have found great dishes. Typically, a proper combination of exploration and exploitation makes the best strategy.

## Prediction with Expert Advice

A well-studied case in online learning is called **prediction with expert advice**. In the example above, you can imagine that you have some websites or apps that show the reviews from other users. You can make better decisions by using the information from the apps. In online learning, these are called *experts*. Say we have a set of $N$ experts $$H=\{h_1,\dots,h_N\}$$. At each timestep, the experts makes their predictions based on the observation $$\hat y_i^{(t)}=h_i(x^{(t)})$$. The online learner receives the predictions, analyzes the expert advice, and generates its own prediction $\hat y^{(t)}$. Sometimes we also call the experts *hypotheses*.

Besides the total loss, we often care about the **regret** in online learning as well. Given the experts $H$, the regret of an online learner (relative to the experts) is defined as:

$$
R^{(T)}(H)=\sum_{t=1}^T l\left(\hat y^{(t)},y^{(t)}\right)-\min_{h^*\in H}\sum_{t=1}^Tl\left(h^*(x^{(t)}),y^{(t)}\right)
$$

In other words, the regret indicates how good the online learner performs relative to the best expert in hindsight. A good online learning algorithm should perform almost the same as the best expert. What we wan to achieve is that the regret grows sub-linearly with $T$, or the average regret converges to zero:

$$
\lim_{T\to\infty}\frac{1}{T}R^{(T)}(H)=0
$$

If this condition is satisfied, we call the online learning algorithm *no regret*. In online learning we do not judge the algorithms too harshly:

- It is possible that the online learner performs better than all the experts, and the regret may be negative. But an online learner which performs *as well as* the best expert is already considered satisfactory.
- Sometimes you feel so unlucky that everything goes wrong. In an *adversarial* environment, the true answer may be deliberately selected by the environment to maximize your loss, so the no regret algorithm is not even achievable. We may need to add a constraint on the adversarial environment, such that the true answer is always generated by some fixed hypothesis in $H$: $$y^{(t)}=h^*(x^{(t)})\ (h^*\in H)$$. This restriction is called *realizability*.

Next let's see some algorithms for online learning with expert advice. For simplicity, we will consider such an environment in the following part:

- The true answer will be returned to the online learner at the end of each timestep.
- The prediction and true answer are binary: $$\hat y^{(t)}\in\{0,1\},y^{(t)}\in\{0,1\}$$.
- The loss is the zero-one loss between the prediction and true answer. The total loss will be the number of mistakes that the online learner makes.

And we make these assumptions about the experts:

- The hypothesis class is finite: $$|H|=N<\infty$$.
- Realizability: There exists a hypothesis $$h^*\in H$$ which is always gives the true answer $$y^{(t)}=h^*(x^{(t)})$$.

## Greedy

The idea of our first **greedy** algorithm is fairly simple: Only keep the experts whose predictions are correct, and throw away the bad ones. We can keep a record of the set of the experts whose answers consistent with the true answer up to now, and follow the advice of one expert in this set. We call this set the *version space*, which is initialized as all experts and shrinks over time. The pseudo-code looks like:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/online-learning-1-1.png">
    </div>
</div>

At each timestep, we receive an observation from the environment. We simply choose the first expert in our current version space, and follow its prediction. When we receive the true answer of this timestep, we update hte version space such that only the experts with correct advice are preserved.

Now we can analyze the error bound of the greedy algorithm. Let $$M^{(t)}=\sum_{i=1}^{t-1}l(\hat y^{(t)},y^{(t)})$$ be the number of mistakes that the online learner makes before time $$t$$. We can consider the lower and upper bounds of the size of the version space $$V^{(t)}$$.

Initially the version space is $$V^{(1)}=H$$, so:

$$
|V^{(1)}|=|H|=N
$$

If we make a mistake at time $t$, that implies the expert which we are following is wrong and should be excluded from the version space:

$$
|V^{(t+1)}|\le|V^{(t)}|-1
$$

By induction, we know:

$$
|V^{(t)}|\le |V^{(1)}|-M^{(t)}=N-M^{(t)}
$$

This is the upper bound of the version space size. For the lower bound, we know the fact that there exists at least one hypothesis whose predictions are always correct (realizability). The hypothesis will never be removed from the version space, so:

$$
|V^{(t)}|\ge 1
$$

Combining the lower and upper bounds, we can derive the error bound:

$$
\begin{aligned}
1\le|V^{(t)}|&\le N-M^{(t)}\\
M^{(t)}&\le N-1
\end{aligned}
$$

Therefore, the number of mistakes is bounded by the number of hypotheses, and it is not dependent on the number of timesteps. When the number of timesteps goes to infinity, we can have a *no regret* algorithm.

In this derivation, we show a common technique for finding the error bound in online learning:

- First we define a *potential function*. In this algorithm, it refers to the size of the version space.
- Then we find the lower and upper bounds of the potential function. They should involve the errors that we make in the process.
- By combining the two bounds together, we can arrive at an inequality which leads to the error bound.

## Halving

As you may have noticed in the greedy algorithm, we are simply following the advice of *the first expert* in the current version space. In the worst case, we may have to exclude the bad experts one by one. Yes, there should be a better strategy to make the decision.

The **halving** algorithm performs better by using more information. We still keep updating the version space, but when making our own prediction, we consider the majority of the expert advice instead. Each expert has one vote, and we will choose the option which takes more than *half* (in our binary environment) of the votes. The pseudo-code looks like:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/online-learning-1-2.png">
    </div>
</div>

The potential function and its lower bound are the same as the greedy algorithm, but we can get a tighter upper bound. If we make a mistake at time $t$, that indicates at least *half* of the experts in the current version space are wrong. After that, no more than half of the experts will remain in the version space:

$$
|V^{(t+1)}|\le\frac{1}{2}|V^{(t)}|
$$

By induction, we can have:

$$
|V^{(t)}|\le \left(\frac{1}{2}\right)^{M^{(t)}}|V^{(1)}|=\frac{N}{2^{M^{(t)}}}
$$

Combine the two bounds together:

$$
\begin{aligned}
1\le|V^{(t)}|&\le \frac{N}{2^{M^{(t)}}}\\
M^{(t)}&\le \log_2 N
\end{aligned}
$$

Compared with the greedy algorithm, the halving algorithm can improve the error bound logarithmically.

## Randomized Greedy

An interesting variant of the greedy algorithm is the **randomized greedy** algorithm. Instead of following the first expert in the current version space, the randomized greedy algorithm uniformly samples one expert and follows its advice. Surprisingly, the randomized greedy algorithm can sometimes perform better than the halving algorithm. The pseudo-code looks like:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/online-learning-1-3.png">
    </div>
</div>

Because of the random sampling part, we can only derive an *expectation* error bound. Let's define

$$
\alpha^{(t)}=\frac{|V^{(t+1)}|}{|V^{(t)}|}
$$

as the ratio between two consecutive version space sizes. $\alpha^{(t)}$ indicates the fraction of experts who will make correct prediction at timestep $t$ in the version space $V^{(t)}$. It can also be interpreted as the probability of picking a good expert and not making a mistake at timestep $t$. In other words, the expected mistake is:

$$
\begin{aligned}
\mathbb{E}[l(\hat y^{(t)},y^{(t)})]&=1-\alpha^{(t)}\\
\mathbb{E}[M^{(T)}]&=\sum_{t=1}^{T-1}(1-\alpha^{(t)})
\end{aligned}
$$

By induction, we can also write the size of the version space as:

$$
\begin{aligned}
|V^{(1)}|&=N\\
|V^{(2)}|&=N\alpha^{(1)}\\
|V^{(T+1)}|&=N\prod_{t=1}^T\alpha^{(t)}
\end{aligned}
$$

We need another inequality $$x\le e^{x-1}\ (\forall x\in\mathbb{R})$$ to relax this expression:

$$
\begin{aligned}
|V^{(T+1)}|&=N\prod_{t=1}^T\alpha^{(t)}\\
&\le N\prod_{t=1}^T e^{\alpha^{(t)}-1}\\
&=Ne^{\sum_{t=1}^T(\alpha^{(t)}-1)}\\
&=Ne^{-\mathbb{E}[M^{(T+1)}]}
\end{aligned}
$$

The lower bound is the same. Combining them together, we have:

$$
\begin{aligned}
1\le|V^{(T)}|&\le Ne^{-\mathbb{E}[M^{(T)}]}\\
\mathbb{E}[M^{(T)}]&\le \ln N
\end{aligned}
$$

The final result seems better than the halving algorithm, since $\ln N\le \log_2 N$. But please note that we are bounding the *expected* number of total mistakes. In the worst case, we may still make $N-1$ mistakes before finding the optimal hypothesis. The benifit is that, the *randomness* can help us defend the *adversarial* environment to some extent, because it prevents the adversarial environment from knowing our decision and selecting the loss accordingly.